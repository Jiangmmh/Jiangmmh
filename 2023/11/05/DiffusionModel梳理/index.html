<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Record what i learn, thought"><title>DiffusionModel梳理 | Minghan's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><div class="darkmode-toggle">🌓</div><script>var prefersDarkMode = window.matchMedia('(prefers-color-scheme: dark)');
var toggle = document.querySelector('.darkmode-toggle');
var html = document.querySelector('html');

html.dataset.dark = localStorage.dark || prefersDarkMode.matches;

toggle.addEventListener('click', () => {
localStorage.dark = !(html.dataset.dark == 'true');
html.dataset.dark = localStorage.dark;
});</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">DiffusionModel梳理</h1><a id="logo" href="/.">Minghan's Blog</a><p class="description">每天进步一点点</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">DiffusionModel梳理</h1><div class="post-meta">2023-11-05<span> | </span><span class="category"><a href="/categories/AI-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">AI-生成模型</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#DDPM"><span class="toc-number">1.</span> <span class="toc-text">DDPM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Improved-DDPM"><span class="toc-number">2.</span> <span class="toc-text">Improved DDPM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffusion-beats-GAN"><span class="toc-number">3.</span> <span class="toc-text">Diffusion beats GAN</span></a></li></ol></div></div><div class="post-content"><h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><p>两个过程——前向过程和反向过程，前向过程是一个高斯核马尔可夫链，实现对数据加噪：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">noise_images</span>(<span class="params">self, x, t</span>):   <span class="comment"># 扩散过程</span></span><br><span class="line">    sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">    sqrt_one_minus_alpha_hat = torch.sqrt(<span class="number">1</span> - self.alpha_hat[t])[:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">    Ɛ = torch.randn_like(x)</span><br><span class="line">    <span class="keyword">return</span> sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ</span><br></pre></td></tr></table></figure>
<p>当正向过程中加入噪声的强度(方差$\beta_t$)足够小时，反向过程也是一个高斯核马尔可夫链，其参数有两个均值$\mu$和方差$\delta$，方差固定，用Unet预测均值，而经过进一步推导，将预测均值转化为预测噪声$\epsilon$，损失函数为MSE。数据生成过程为从先验分布（标准高斯分布）中采样，然后根据该马尔可夫链采样逐步降噪。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, model, n</span>):     <span class="comment"># Reverse Process</span></span><br><span class="line">    logging.info(<span class="string">f&quot;Sampling <span class="subst">&#123;n&#125;</span> new images....&quot;</span>)</span><br><span class="line">    model.<span class="built_in">eval</span>()    <span class="comment"># 测试模式，不启用Batch-Norm和Dropout</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        x = torch.randn((n, <span class="number">3</span>, self.img_size, self.img_size)).to(self.device) <span class="comment"># 产生噪声图</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">1</span>, self.noise_steps)), position=<span class="number">0</span>): <span class="comment"># 从T~1</span></span><br><span class="line">            t = (torch.ones(n) * i).long().to(self.device)</span><br><span class="line">            predicted_noise = model(x, t)  <span class="comment"># 预测噪声</span></span><br><span class="line">            alpha = self.alpha[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            alpha_hat = self.alpha_hat[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            beta = self.beta[t][:, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">1</span>:</span><br><span class="line">                noise = torch.randn_like(x)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                noise = torch.zeros_like(x)</span><br><span class="line">            <span class="comment"># 高斯分布，前半部分为均值，后半部分为方差</span></span><br><span class="line">            x = <span class="number">1</span> / torch.sqrt(alpha) * (x - (beta / torch.sqrt(<span class="number">1</span> - alpha_hat)) * predicted_noise) + torch.sqrt(beta) * noise</span><br><span class="line">    <span class="comment"># 将输出变为图像</span></span><br><span class="line">    model.train()</span><br><span class="line">    x = (x.clamp(-<span class="number">1</span>, <span class="number">1</span>) + <span class="number">1</span>) / <span class="number">2</span>  <span class="comment"># clamp将x中的数据挤压在[-1, 1]，超出的值用边界代替</span></span><br><span class="line">    x = (x * <span class="number">255</span>).<span class="built_in">type</span>(torch.uint8)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>所用的Unet模型如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UNet_conditional</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, c_in=<span class="number">3</span>, c_out=<span class="number">3</span>, time_dim=<span class="number">256</span>, num_classes=<span class="literal">None</span>, device=<span class="string">&quot;cuda&quot;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.device = device</span><br><span class="line">        self.time_dim = time_dim</span><br><span class="line">        self.inc = DoubleConv(c_in, <span class="number">64</span>)</span><br><span class="line">        self.down1 = Down(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">        self.sa1 = SelfAttention(<span class="number">128</span>, <span class="number">32</span>)</span><br><span class="line">        self.down2 = Down(<span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">        self.sa2 = SelfAttention(<span class="number">256</span>, <span class="number">16</span>)</span><br><span class="line">        self.down3 = Down(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">        self.sa3 = SelfAttention(<span class="number">256</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">        self.bot1 = DoubleConv(<span class="number">256</span>, <span class="number">512</span>)</span><br><span class="line">        self.bot2 = DoubleConv(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        self.bot3 = DoubleConv(<span class="number">512</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.up1 = Up(<span class="number">512</span>, <span class="number">128</span>)</span><br><span class="line">        self.sa4 = SelfAttention(<span class="number">128</span>, <span class="number">16</span>)</span><br><span class="line">        self.up2 = Up(<span class="number">256</span>, <span class="number">64</span>)</span><br><span class="line">        self.sa5 = SelfAttention(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.up3 = Up(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.sa6 = SelfAttention(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        self.outc = nn.Conv2d(<span class="number">64</span>, c_out, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> num_classes <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.label_emb = nn.Embedding(num_classes, time_dim) <span class="comment"># 引入类别条件</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pos_encoding</span>(<span class="params">self, t, channels</span>):</span><br><span class="line">        inv_freq = <span class="number">1.0</span> / (</span><br><span class="line">            <span class="number">10000</span></span><br><span class="line">            ** (torch.arange(<span class="number">0</span>, channels, <span class="number">2</span>, device=self.device).<span class="built_in">float</span>() / channels)</span><br><span class="line">        )</span><br><span class="line">        pos_enc_a = torch.sin(t.repeat(<span class="number">1</span>, channels // <span class="number">2</span>) * inv_freq)</span><br><span class="line">        pos_enc_b = torch.cos(t.repeat(<span class="number">1</span>, channels // <span class="number">2</span>) * inv_freq)</span><br><span class="line">        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> pos_enc</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, t, y</span>):</span><br><span class="line">        t = t.unsqueeze(-<span class="number">1</span>).<span class="built_in">type</span>(torch.<span class="built_in">float</span>)</span><br><span class="line">        t = self.pos_encoding(t, self.time_dim) <span class="comment"># 引入时间embedding</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            t += self.label_emb(y)  <span class="comment"># 引入条件embedding</span></span><br><span class="line"></span><br><span class="line">        x1 = self.inc(x)</span><br><span class="line">        x2 = self.down1(x1, t)</span><br><span class="line">        x2 = self.sa1(x2)</span><br><span class="line">        x3 = self.down2(x2, t)</span><br><span class="line">        x3 = self.sa2(x3)</span><br><span class="line">        x4 = self.down3(x3, t)</span><br><span class="line">        x4 = self.sa3(x4)</span><br><span class="line"></span><br><span class="line">        x4 = self.bot1(x4)</span><br><span class="line">        x4 = self.bot2(x4)</span><br><span class="line">        x4 = self.bot3(x4)</span><br><span class="line"></span><br><span class="line">        x = self.up1(x4, x3, t)</span><br><span class="line">        x = self.sa4(x)</span><br><span class="line">        x = self.up2(x, x2, t)</span><br><span class="line">        x = self.sa5(x)</span><br><span class="line">        x = self.up3(x, x1, t)</span><br><span class="line">        x = self.sa6(x)</span><br><span class="line">        output = self.outc(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>

<h2 id="Improved-DDPM"><a href="#Improved-DDPM" class="headerlink" title="Improved DDPM"></a>Improved DDPM</h2><p>关键点：</p>
<ul>
<li>学习方差</li>
<li>consine schedule作为噪声策略</li>
<li>importance sampling</li>
</ul>
<p>反向过程的方差为前后向过程的插值，其中$v$为要学习参数。<br>$$\sum_\theta(x_t, t)&#x3D;exp(vlog\beta_t+(1-v)log\widetilde{\beta_t})$$</p>
<p>损失函数如下，其中$L_{simple}$指导均值，$L_{vlb}$指导方差。<br>$$L_{hybrid}&#x3D;L_{simple}+\lambda L_{vlb}$$</p>
<p>Improtance sampling使学习曲线变得更加平滑。</p>
<h2 id="Diffusion-beats-GAN"><a href="#Diffusion-beats-GAN" class="headerlink" title="Diffusion beats GAN"></a>Diffusion beats GAN</h2><p>这篇文章的作者做了大量的消融实验，改进了网络结构和一些超参数的设置。<br><img src="/images/dbgan.png"><br><img src="/images/dbgan2.png"></p>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>DiffusionModel梳理</p><p><span>文章作者：</span>minghan</p><p><span>发布时间：</span>2023-11-05</p><p><span>最后更新：</span>2023-11-05</p><p><span>原始链接：</span><a href="/2023/11/05/DiffusionModel梳理/">http://jiangmmh.github.io/2023/11/05/DiffusionModel%E6%A2%B3%E7%90%86/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="http://jiangmmh.github.io/2023/11/05/DiffusionModel%E6%A2%B3%E7%90%86/"></i></span></p><p><span>版权声明：</span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0 CN</a> 许可协议。转载请注明出处！</p></div><br><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/diffusion/" rel="tag">diffusion</a></li></ul></div><div class="post-nav"><a class="next" href="/2023/11/03/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E6%89%98%E7%AE%A1%E5%88%B0githubpage/">使用hexo搭建个人博客并托管到githubpage</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://Jiangmmh.github.io"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AI-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">AI-生成模型</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%96%87%E5%8C%96-%E8%AF%97%E8%AF%8D/">文化-诗词</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BD%91%E7%AB%99-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/">网站-个人博客</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E8%AF%97%E8%AF%8D/" style="font-size: 15px;">诗词</a> <a href="/tags/blog/" style="font-size: 15px;">blog</a> <a href="/tags/diffusion/" style="font-size: 15px;">diffusion</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/11/05/DiffusionModel%E6%A2%B3%E7%90%86/">DiffusionModel梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/11/03/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E5%B9%B6%E6%89%98%E7%AE%A1%E5%88%B0githubpage/">使用hexo搭建个人博客并托管到githubpage</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/11/03/%E9%80%81%E4%B8%9C%E9%98%B3%E9%A9%AC%E7%94%9F%E5%BA%8F/">送东阳马生序</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/11/02/%E8%AF%97%E6%AD%8C-%E6%B5%8B%E8%AF%95/">沁园春·雪</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/11/01/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">Minghan's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>